# SportsProject

Tennis Match Predictor Project:

In this project we can find the following documentes, 3 datasets and 6 notebooks: atp_tennis.csv, tennis_matches_with_features.csv and tennis_matches_enriched.csv are the datasets we will work with. The first one atp_tennis.csv is the one we downloaded from kaggle and the 2 others are enriched datasets where we worked with formats, missing values and added columns with features that we thought that might help our prediction algorithms such as H2H or winRateSurface.

The main objective was to get familiar with different ML algorithms and see which one would work better in this kind of context. First of all we have the data.ipynb where is the code to edit and add features to the dataset. The predictors we finally used were the following.     "playerCode", "opponentCode","Rank_1", "Rank_2", "Pts_1", "Pts_2","h2h_matches", "h2h_win_pct", "h2h_surface_matches", "h2h_surface_win_pct","recent_matches_p1", "recent_win_pct_p1", "recent_avg_opp_rank_p1", "recent_matches_p2", "recent_win_pct_p2", "recent_avg_opp_rank_p2", "win_pct_surface_p1", "win_pct_surface_p2","series_level", "round_num", "best_of","implied_prob_p1", "implied_prob_p2",

The first model we implemented was RandomForest. We implemented it 2 times because the first time we used features which didn't bring imformation to the algorithm so we did agian with the enriched dataset. This model was chosen for its ability to handle complex feature interactions in tennis match prediction. We implemented an optimized Random Forest with 400 trees, using key features like player rankings, head-to-head records (both overall and surface-specific), recent performance metrics, and surface win percentages. After hyperparameter tuning via GridSearch, the model achieved 67.9% test accuracy on 2023-2024 matches. The most relevant predictor was ODD, which is a human factor (beting) which really surprised us. In this document we also did a matchPrediction function were you can pass two players and a surface and it returns their percentage. This is just visual in the other algorithms we didn't implement it we were just looking for the test accuracy.  

We then implemented XGBoost for its superior handling of imbalanced tennis data through gradient boosting. After randomized hyperparameter tuning, the optimal configuration used 700 trees with shallow depth (max_depth=4) to prevent overfitting, alongside aggressive regularization (gamma=5, min_child_weight=10). The test result gave us very similar results to the RF and no significant improve was seen. This is also logic because both algorithms are simmilar the main difference is that RF does parallel trees while XGBoost does it sequentialy.

We also tested CatBoost, a gradient boosting model particularly well-suited for categorical data, which is very present in our dataset. We took advantage of its native handling of categorical variables (like player IDs or tournament rounds) without needing to encode them manually. After tuning with moderate depth (depth=10), a low learning rate (0.02), and early stopping, the model achieved a test accuracy of 0.684, slightly higher than both RF and XGB. The main advantage seems to come from CatBoostâ€™s treatment of categorical variables, which are highly informative in this context. This allowed the model to better generalize on unseen players or matchups without overfitting.

Finally, we explored TabNet, a deep learning model specifically designed for tabular data. We tuned it using Optuna to find optimal architectural and training parameters, including embedding sizes, learning rate, and number of steps. Although it took longer to train, TabNet achieved an accuracy of around 0.682, which is comparable to XGB and CB. Interestingly, despite being a deep model, it did not outperform boosting methods. This might be due to the relatively small size of the dataset for a neural network, or because boosting trees are simply better suited for this structured type of data with mixed feature types.

